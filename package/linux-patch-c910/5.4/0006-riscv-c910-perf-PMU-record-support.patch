From 7a00f34e85a794691914a7ee1d4ec0d3bc0e4b89 Mon Sep 17 00:00:00 2001
From: Mao Han <han_mao@linux.alibaba.com>
Date: Thu, 7 May 2020 09:48:35 +0800
Subject: [PATCH] riscv/c910: perf PMU record support

This patch add basic perf counter access and hw event sampling support
for csky c910. The PMU can support various hardware event, including
cycles, instructions, cache access/miss, LSU event and etc.

Liu Yibin init the counters of c910.

Mao Han add counters' overflow interrupts for PMU record.

Signed-off-by: Mao Han <han_mao@c-sky.com>
Co-developed: Liu Yibin <yibin_liu@c-sky.com>
Reviewed-by: Guo Ren <ren_guo@c-sky.com>
---
 arch/riscv/include/asm/csr.h        |   2 +
 arch/riscv/include/asm/perf_event.h |   4 +-
 arch/riscv/include/asm/sbi.h        |   6 +
 arch/riscv/kernel/irq.c             |   7 +
 drivers/perf/Kconfig                |   8 +
 drivers/perf/Makefile               |   1 +
 drivers/perf/c910_pmu.c             | 756 ++++++++++++++++++++++++++++
 include/linux/cpuhotplug.h          |   1 +
 8 files changed, 783 insertions(+), 2 deletions(-)
 create mode 100644 drivers/perf/c910_pmu.c

diff --git a/arch/riscv/include/asm/csr.h b/arch/riscv/include/asm/csr.h
index c747d693e..f38383836 100644
--- a/arch/riscv/include/asm/csr.h
+++ b/arch/riscv/include/asm/csr.h
@@ -59,6 +59,7 @@
 #define IRQ_U_EXT		8
 #define IRQ_S_EXT		9
 #define IRQ_M_EXT		11
+#define IRQ_S_PMU		17
 
 #define EXC_INST_MISALIGNED	0
 #define EXC_INST_ACCESS		1
@@ -74,6 +75,7 @@
 #define SIE_SSIE		(_AC(0x1, UL) << IRQ_S_SOFT)
 #define SIE_STIE		(_AC(0x1, UL) << IRQ_S_TIMER)
 #define SIE_SEIE		(_AC(0x1, UL) << IRQ_S_EXT)
+#define SIE_SMIE		(_AC(0x1, UL) << IRQ_S_PMU)
 
 #define CSR_CYCLE		0xc00
 #define CSR_TIME		0xc01
diff --git a/arch/riscv/include/asm/perf_event.h b/arch/riscv/include/asm/perf_event.h
index aefbfaa6a..21b719f44 100644
--- a/arch/riscv/include/asm/perf_event.h
+++ b/arch/riscv/include/asm/perf_event.h
@@ -18,8 +18,8 @@
  * The RISCV_MAX_COUNTERS parameter should be specified.
  */
 
-#ifdef CONFIG_RISCV_BASE_PMU
-#define RISCV_MAX_COUNTERS	2
+#if defined(CONFIG_RISCV_BASE_PMU) || defined(CONFIG_PERF_C910_PMU)
+#define RISCV_MAX_COUNTERS	32
 #endif
 
 #ifndef RISCV_MAX_COUNTERS
diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h
index 21134b3ef..3f40aec76 100644
--- a/arch/riscv/include/asm/sbi.h
+++ b/arch/riscv/include/asm/sbi.h
@@ -17,6 +17,7 @@
 #define SBI_REMOTE_SFENCE_VMA 6
 #define SBI_REMOTE_SFENCE_VMA_ASID 7
 #define SBI_SHUTDOWN 8
+#define SBI_PMU 0x09000001
 
 #define SBI_CALL(which, arg0, arg1, arg2, arg3) ({		\
 	register uintptr_t a0 asm ("a0") = (uintptr_t)(arg0);	\
@@ -94,4 +95,9 @@ static inline void sbi_remote_sfence_vma_asid(const unsigned long *hart_mask,
 	SBI_CALL_4(SBI_REMOTE_SFENCE_VMA_ASID, hart_mask, start, size, asid);
 }
 
+static inline void sbi_set_pmu(int start)
+{
+	SBI_CALL_1(SBI_PMU, start);
+}
+
 #endif
diff --git a/arch/riscv/kernel/irq.c b/arch/riscv/kernel/irq.c
index fffac6ddb..2f38b05ec 100644
--- a/arch/riscv/kernel/irq.c
+++ b/arch/riscv/kernel/irq.c
@@ -17,6 +17,7 @@
 #define INTERRUPT_CAUSE_SOFTWARE	IRQ_S_SOFT
 #define INTERRUPT_CAUSE_TIMER		IRQ_S_TIMER
 #define INTERRUPT_CAUSE_EXTERNAL	IRQ_S_EXT
+#define INTERRUPT_CAUSE_PMU		IRQ_S_PMU
 
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
@@ -24,6 +25,7 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 	return 0;
 }
 
+extern int riscv_pmu_handle_irq(void);
 asmlinkage __visible void __irq_entry do_IRQ(struct pt_regs *regs)
 {
 	struct pt_regs *old_regs = set_irq_regs(regs);
@@ -33,6 +35,11 @@ asmlinkage __visible void __irq_entry do_IRQ(struct pt_regs *regs)
 	case INTERRUPT_CAUSE_TIMER:
 		riscv_timer_interrupt();
 		break;
+#ifdef CONFIG_C910_PMU
+	case INTERRUPT_CAUSE_PMU:
+		riscv_pmu_handle_irq();
+		break;
+#endif
 #ifdef CONFIG_SMP
 	case INTERRUPT_CAUSE_SOFTWARE:
 		/*
diff --git a/drivers/perf/Kconfig b/drivers/perf/Kconfig
index 09ae8a970..a2af6235c 100644
--- a/drivers/perf/Kconfig
+++ b/drivers/perf/Kconfig
@@ -71,6 +71,14 @@ config ARM_DSU_PMU
 	  system, control logic. The PMU allows counting various events related
 	  to DSU.
 
+config C910_PMU
+	bool "C910 Performance Monitoring Unit"
+	depends on RISCV
+	def_bool y
+	help
+	  C910 PMU that support various hardware event, including cycles,
+	  instructions, cache access/miss, LSU event and etc.
+
 config FSL_IMX8_DDR_PMU
 	tristate "Freescale i.MX8 DDR perf monitor"
 	depends on ARCH_MXC
diff --git a/drivers/perf/Makefile b/drivers/perf/Makefile
index 2ebb4de17..a0242349a 100644
--- a/drivers/perf/Makefile
+++ b/drivers/perf/Makefile
@@ -12,3 +12,4 @@ obj-$(CONFIG_QCOM_L3_PMU) += qcom_l3_pmu.o
 obj-$(CONFIG_THUNDERX2_PMU) += thunderx2_pmu.o
 obj-$(CONFIG_XGENE_PMU) += xgene_pmu.o
 obj-$(CONFIG_ARM_SPE_PMU) += arm_spe_pmu.o
+obj-$(CONFIG_C910_PMU) += c910_pmu.o
diff --git a/drivers/perf/c910_pmu.c b/drivers/perf/c910_pmu.c
new file mode 100644
index 000000000..6b6be2feb
--- /dev/null
+++ b/drivers/perf/c910_pmu.c
@@ -0,0 +1,756 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd. */
+
+#include <linux/errno.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/perf_event.h>
+#include <linux/platform_device.h>
+#include <linux/smp.h>
+#include <asm/perf_event.h>
+#include <asm/sbi.h>
+
+#define RISCV_PMU_CYCLE		0
+#define RISCV_PMU_TIME		1
+#undef RISCV_PMU_INSTRET
+#define RISCV_PMU_INSTRET	2
+#define RISCV_PMU_L1ICAC	3	/* ICache Access */
+#define RISCV_PMU_L1ICMC	4	/* ICache Miss */
+#define RISCV_PMU_IUTLBMC	5	/* I-UTLB Miss */
+#define RISCV_PMU_DUTLBMC	6	/* D-UTLB Miss */
+#define RISCV_PMU_JTLBMC	7	/* JTLB Miss Counter */
+
+#define RISCV_PMU_CBMC		8	/* Cond-br-mispredict */
+#define RISCV_PMU_CBIC		9	/* Cond-br-instruction */
+#define RISCV_PMU_IBMC		10	/* Indirect Branch Mispredict */
+#define RISCV_PMU_IBIC		11	/* Indirect Branch Instruction */
+#define RISCV_PMU_LSUSFC	12	/* LSU Spec Fail */
+#define RISCV_PMU_STC		13	/* Store Instruction */
+
+#define RISCV_PMU_L1DCRAC	14	/* L1 DCache Read Access */
+#define RISCV_PMU_L1DCRMC	15	/* L1 DCache Read Miss */
+#define RISCV_PMU_L1DCWAC	16	/* L1 DCache Write Access */
+#define RISCV_PMU_L1DCWMC	17	/* L1 DCache Write Miss */
+
+#define RISCV_PMU_L2CRAC	18	/* L2 Cache Read Access */
+#define RISCV_PMU_L2CRMC	19	/* L2 Cache Read Miss */
+#define RISCV_PMU_L2CWAC	20	/* L2 Cache Write Access */
+#define RISCV_PMU_L2CWMC	21	/* L2 Cache Write Miss */
+
+#define RISCV_PMU_RFLFC		22	/* RF Launch Fail */
+#define RISCV_PMU_RFRLFC	23	/* RF Reg Launch Fail */
+#define RISCV_PMU_RFIC		24	/* RF Instruction */
+
+#define RISCV_PMU_LSUC4SC	25	/* LSU Cross 4K Stall */
+#define RISCV_PMU_LSUOSC	26	/* LSU Other Stall */
+#define RISCV_PMU_LSUSQDC	27	/* LSU SQ Discard */
+#define RISCV_PMU_LSUSQDDC	28	/* LSU SQ Data Discard */
+
+#define SCOUNTERINTEN		0x5c4
+#define SCOUNTEROF		0x5c5
+#define SCOUNTERBASE		0x5e0
+
+#define WRITE_COUNTER(idx, value) \
+		csr_write(SCOUNTERBASE + idx, value)
+
+/* The events for a given PMU register set. */
+struct pmu_hw_events {
+	/*
+	 * The events that are active on the PMU for the given index.
+	 */
+	struct perf_event *events[RISCV_MAX_COUNTERS];
+
+	/*
+	 * A 1 bit for an index indicates that the counter is being used for
+	 * an event. A 0 means that the counter can be used.
+	 */
+	unsigned long used_mask[BITS_TO_LONGS(RISCV_MAX_COUNTERS)];
+};
+
+static struct riscv_pmu_t {
+	struct pmu		      pmu;
+	struct pmu_hw_events __percpu   *hw_events;
+	struct platform_device	  *plat_device;
+	u64			     max_period;
+} riscv_pmu;
+
+/*
+ * Hardware & cache maps and their methods
+ */
+
+static const int riscv_hw_event_map[] = {
+	[PERF_COUNT_HW_CPU_CYCLES]			= RISCV_PMU_CYCLE,
+	[PERF_COUNT_HW_INSTRUCTIONS]			= RISCV_PMU_INSTRET,
+
+	[PERF_COUNT_HW_CACHE_REFERENCES]		= RISCV_PMU_L1ICAC,
+	[PERF_COUNT_HW_CACHE_MISSES]			= RISCV_PMU_L1ICMC,
+
+	[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]		= RISCV_PMU_CBIC,
+	[PERF_COUNT_HW_BRANCH_MISSES]			= RISCV_PMU_CBMC,
+
+	[PERF_COUNT_HW_BUS_CYCLES]			= RISCV_PMU_IBMC,
+	[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]		= RISCV_PMU_IBIC,
+	[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]		= RISCV_PMU_LSUSFC,
+	[PERF_COUNT_HW_REF_CPU_CYCLES]			= RISCV_PMU_STC,
+};
+
+#define C(x) PERF_COUNT_HW_CACHE_##x
+static const int riscv_cache_event_map[PERF_COUNT_HW_CACHE_MAX]
+[PERF_COUNT_HW_CACHE_OP_MAX]
+[PERF_COUNT_HW_CACHE_RESULT_MAX] = {
+	[C(L1D)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_PMU_L1DCRAC,
+			[C(RESULT_MISS)] = RISCV_PMU_L1DCRMC,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_PMU_L1DCWAC,
+			[C(RESULT_MISS)] = RISCV_PMU_L1DCWMC,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(L1I)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(LL)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_PMU_L2CRAC,
+			[C(RESULT_MISS)] = RISCV_PMU_L2CRMC,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_PMU_L2CWAC,
+			[C(RESULT_MISS)] = RISCV_PMU_L2CWMC,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(DTLB)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(ITLB)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(BPU)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+	[C(NODE)] = {
+		[C(OP_READ)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_WRITE)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+		[C(OP_PREFETCH)] = {
+			[C(RESULT_ACCESS)] = RISCV_OP_UNSUPP,
+			[C(RESULT_MISS)] = RISCV_OP_UNSUPP,
+		},
+	},
+};
+
+/*
+ * Low-level functions: reading/writing counters
+ */
+static inline u64 read_counter(int idx)
+{
+	u64 val = 0;
+
+	switch (idx) {
+	case RISCV_PMU_CYCLE:
+		val = csr_read(cycle);
+		break;
+	case RISCV_PMU_INSTRET:
+		val = csr_read(instret);
+		break;
+	case RISCV_PMU_L1ICAC:
+		val = csr_read(hpmcounter3);
+		break;
+	case RISCV_PMU_L1ICMC:
+		val = csr_read(hpmcounter4);
+		break;
+	case RISCV_PMU_IUTLBMC:
+		val = csr_read(hpmcounter5);
+		break;
+	case RISCV_PMU_DUTLBMC:
+		val = csr_read(hpmcounter6);
+		break;
+	case RISCV_PMU_JTLBMC:
+		val = csr_read(hpmcounter7);
+		break;
+	case RISCV_PMU_CBMC:
+		val = csr_read(hpmcounter8);
+		break;
+	case RISCV_PMU_CBIC:
+		val = csr_read(hpmcounter9);
+		break;
+	case RISCV_PMU_IBMC:
+		val = csr_read(hpmcounter10);
+		break;
+	case RISCV_PMU_IBIC:
+		val = csr_read(hpmcounter11);
+		break;
+	case RISCV_PMU_LSUSFC:
+		val = csr_read(hpmcounter12);
+		break;
+	case RISCV_PMU_STC:
+		val = csr_read(hpmcounter13);
+		break;
+	case RISCV_PMU_L1DCRAC:
+		val = csr_read(hpmcounter14);
+		break;
+	case RISCV_PMU_L1DCRMC:
+		val = csr_read(hpmcounter15);
+		break;
+	case RISCV_PMU_L1DCWAC:
+		val = csr_read(hpmcounter16);
+		break;
+	case RISCV_PMU_L1DCWMC:
+		val = csr_read(hpmcounter17);
+		break;
+	case RISCV_PMU_L2CRAC:
+		val = csr_read(hpmcounter18);
+		break;
+	case RISCV_PMU_L2CRMC:
+		val = csr_read(hpmcounter19);
+		break;
+	case RISCV_PMU_L2CWAC:
+		val = csr_read(hpmcounter20);
+		break;
+	case RISCV_PMU_L2CWMC:
+		val = csr_read(hpmcounter21);
+		break;
+	case RISCV_PMU_RFLFC:
+		val = csr_read(hpmcounter22);
+		break;
+	case RISCV_PMU_RFRLFC:
+		val = csr_read(hpmcounter23);
+		break;
+	case RISCV_PMU_RFIC:
+		val = csr_read(hpmcounter24);
+		break;
+	case RISCV_PMU_LSUC4SC:
+		val = csr_read(hpmcounter25);
+		break;
+	case RISCV_PMU_LSUOSC:
+		val = csr_read(hpmcounter26);
+		break;
+	case RISCV_PMU_LSUSQDC:
+		val = csr_read(hpmcounter27);
+		break;
+	case RISCV_PMU_LSUSQDDC:
+		val = csr_read(hpmcounter28);
+		break;
+	default:
+		WARN_ON_ONCE(idx < 0 ||	idx > RISCV_MAX_COUNTERS);
+		return -EINVAL;
+	}
+
+	return val;
+}
+
+static inline void write_counter(int idx, u64 value)
+{
+	switch (idx) {
+	case RISCV_PMU_CYCLE:
+		WRITE_COUNTER(RISCV_PMU_CYCLE, value);
+		break;
+	case RISCV_PMU_INSTRET:
+		WRITE_COUNTER(RISCV_PMU_INSTRET, value);
+		break;
+	case RISCV_PMU_L1ICAC:
+		WRITE_COUNTER(RISCV_PMU_L1ICAC, value);
+		break;
+	case RISCV_PMU_L1ICMC:
+		WRITE_COUNTER(RISCV_PMU_L1ICMC, value);
+		break;
+	case RISCV_PMU_IUTLBMC:
+		WRITE_COUNTER(RISCV_PMU_IUTLBMC, value);
+		break;
+	case RISCV_PMU_DUTLBMC:
+		WRITE_COUNTER(RISCV_PMU_DUTLBMC, value);
+		break;
+	case RISCV_PMU_JTLBMC:
+		WRITE_COUNTER(RISCV_PMU_JTLBMC, value);
+		break;
+	case RISCV_PMU_CBMC:
+		WRITE_COUNTER(RISCV_PMU_CBMC, value);
+		break;
+	case RISCV_PMU_CBIC:
+		WRITE_COUNTER(RISCV_PMU_CBIC, value);
+		break;
+	case RISCV_PMU_IBMC:
+		WRITE_COUNTER(RISCV_PMU_IBMC, value);
+		break;
+	case RISCV_PMU_IBIC:
+		WRITE_COUNTER(RISCV_PMU_IBIC, value);
+		break;
+	case RISCV_PMU_LSUSFC:
+		WRITE_COUNTER(RISCV_PMU_LSUSFC, value);
+		break;
+	case RISCV_PMU_STC:
+		WRITE_COUNTER(RISCV_PMU_STC, value);
+		break;
+	case RISCV_PMU_L1DCRAC:
+		WRITE_COUNTER(RISCV_PMU_L1DCRAC, value);
+		break;
+	case RISCV_PMU_L1DCRMC:
+		WRITE_COUNTER(RISCV_PMU_L1DCRMC, value);
+		break;
+	case RISCV_PMU_L1DCWAC:
+		WRITE_COUNTER(RISCV_PMU_L1DCWAC, value);
+		break;
+	case RISCV_PMU_L1DCWMC:
+		WRITE_COUNTER(RISCV_PMU_L1DCWMC, value);
+		break;
+	case RISCV_PMU_L2CRAC:
+		WRITE_COUNTER(RISCV_PMU_L2CRAC, value);
+		break;
+	case RISCV_PMU_L2CRMC:
+		WRITE_COUNTER(RISCV_PMU_L2CRMC, value);
+		break;
+	case RISCV_PMU_L2CWAC:
+		WRITE_COUNTER(RISCV_PMU_L2CWAC, value);
+		break;
+	case RISCV_PMU_L2CWMC:
+		WRITE_COUNTER(RISCV_PMU_L2CWMC, value);
+		break;
+	case RISCV_PMU_RFLFC:
+		WRITE_COUNTER(RISCV_PMU_RFLFC, value);
+		break;
+	case RISCV_PMU_RFRLFC:
+		WRITE_COUNTER(RISCV_PMU_RFRLFC, value);
+		break;
+	case RISCV_PMU_RFIC:
+		WRITE_COUNTER(RISCV_PMU_RFIC, value);
+		break;
+	case RISCV_PMU_LSUC4SC:
+		WRITE_COUNTER(RISCV_PMU_LSUC4SC, value);
+		break;
+	case RISCV_PMU_LSUOSC:
+		WRITE_COUNTER(RISCV_PMU_LSUOSC, value);
+		break;
+	case RISCV_PMU_LSUSQDC:
+		WRITE_COUNTER(RISCV_PMU_LSUSQDC, value);
+		break;
+	case RISCV_PMU_LSUSQDDC:
+		WRITE_COUNTER(RISCV_PMU_LSUSQDDC, value);
+		break;
+	default:
+		WARN_ON_ONCE(idx < 0 ||	idx > RISCV_MAX_COUNTERS);
+	}
+}
+
+int riscv_pmu_event_set_period(struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+	s64 left = local64_read(&hwc->period_left);
+	s64 period = hwc->sample_period;
+	int ret = 0;
+
+	if (unlikely(left <= -period)) {
+		left = period;
+		local64_set(&hwc->period_left, left);
+		hwc->last_period = period;
+		ret = 1;
+	}
+
+	if (unlikely(left <= 0)) {
+		left += period;
+		local64_set(&hwc->period_left, left);
+		hwc->last_period = period;
+		ret = 1;
+	}
+
+	if (left < 0)
+		left = riscv_pmu.max_period;
+
+	/*
+	 * The hw event starts counting from this event offset,
+	 * mark it to be able to extract future "deltas":
+	 */
+	local64_set(&hwc->prev_count, (u64)(-left));
+	csr_write(SCOUNTEROF, csr_read(SCOUNTEROF) & ~BIT(hwc->idx));
+	write_counter(hwc->idx, (u64)(-left));
+
+	perf_event_update_userpage(event);
+
+	return ret;
+}
+
+static void riscv_perf_event_update(struct perf_event *event,
+				   struct hw_perf_event *hwc)
+{
+	uint64_t prev_raw_count = local64_read(&hwc->prev_count);
+	/*
+	 * Sign extend count value to 64bit, otherwise delta calculation
+	 * would be incorrect when overflow occurs.
+	 */
+	uint64_t new_raw_count = read_counter(hwc->idx);
+	int64_t delta = new_raw_count - prev_raw_count;
+
+	/*
+	 * We aren't afraid of hwc->prev_count changing beneath our feet
+	 * because there's no way for us to re-enter this function anytime.
+	 */
+	local64_set(&hwc->prev_count, new_raw_count);
+	local64_add(delta, &event->count);
+	local64_sub(delta, &hwc->period_left);
+}
+
+static void riscv_pmu_read(struct perf_event *event)
+{
+	riscv_perf_event_update(event, &event->hw);
+}
+
+static int riscv_pmu_cache_event(u64 config)
+{
+	unsigned int cache_type, cache_op, cache_result;
+
+	cache_type      = (config >>  0) & 0xff;
+	cache_op	= (config >>  8) & 0xff;
+	cache_result    = (config >> 16) & 0xff;
+
+	if (cache_type >= PERF_COUNT_HW_CACHE_MAX)
+		return -EINVAL;
+	if (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)
+		return -EINVAL;
+	if (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)
+		return -EINVAL;
+
+	return riscv_cache_event_map[cache_type][cache_op][cache_result];
+}
+
+static int riscv_pmu_event_init(struct perf_event *event)
+{
+	struct hw_perf_event *hwc = &event->hw;
+	int ret;
+
+	switch (event->attr.type) {
+	case PERF_TYPE_HARDWARE:
+		if (event->attr.config >= PERF_COUNT_HW_MAX)
+			return -ENOENT;
+		ret = riscv_hw_event_map[event->attr.config];
+		if (ret == RISCV_OP_UNSUPP)
+			return -ENOENT;
+		hwc->idx = ret;
+		break;
+	case PERF_TYPE_HW_CACHE:
+		ret = riscv_pmu_cache_event(event->attr.config);
+		if (ret == RISCV_OP_UNSUPP)
+			return -ENOENT;
+		hwc->idx = ret;
+		break;
+	case PERF_TYPE_RAW:
+		if (event->attr.config < 0 || event->attr.config >
+		    RISCV_MAX_COUNTERS)
+			return -ENOENT;
+		hwc->idx = event->attr.config;
+		break;
+	default:
+		return -ENOENT;
+	}
+
+	return 0;
+}
+
+static void riscv_pmu_enable(struct pmu *pmu)
+{
+}
+
+/* stops all counters */
+static void riscv_pmu_disable(struct pmu *pmu)
+{
+}
+
+static void riscv_pmu_start(struct perf_event *event, int flags)
+{
+	unsigned long flg;
+	struct hw_perf_event *hwc = &event->hw;
+	int idx = hwc->idx;
+
+	if (WARN_ON_ONCE(idx == -1))
+		return;
+
+	if (flags & PERF_EF_RELOAD)
+		WARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));
+
+	hwc->state = 0;
+
+	riscv_pmu_event_set_period(event);
+
+	local_irq_save(flg);
+
+	csr_write(SCOUNTERINTEN, BIT(idx) | csr_read(SCOUNTERINTEN));
+
+	local_irq_restore(flg);
+}
+
+static void riscv_pmu_stop_event(struct perf_event *event)
+{
+	unsigned long flg;
+	struct hw_perf_event *hwc = &event->hw;
+	int idx = hwc->idx;
+
+	local_irq_save(flg);
+
+	csr_write(SCOUNTERINTEN, ~BIT(idx) & csr_read(SCOUNTERINTEN));
+
+	local_irq_restore(flg);
+}
+
+static void riscv_pmu_stop(struct perf_event *event, int flags)
+{
+	if (!(event->hw.state & PERF_HES_STOPPED)) {
+		riscv_pmu_stop_event(event);
+		event->hw.state |= PERF_HES_STOPPED;
+	}
+
+	if ((flags & PERF_EF_UPDATE) &&
+	    !(event->hw.state & PERF_HES_UPTODATE)) {
+		riscv_perf_event_update(event, &event->hw);
+		event->hw.state |= PERF_HES_UPTODATE;
+	}
+}
+
+static void riscv_pmu_del(struct perf_event *event, int flags)
+{
+	struct pmu_hw_events *hw_events = this_cpu_ptr(riscv_pmu.hw_events);
+	struct hw_perf_event *hwc = &event->hw;
+
+	riscv_pmu_stop(event, PERF_EF_UPDATE);
+
+	hw_events->events[hwc->idx] = NULL;
+
+	perf_event_update_userpage(event);
+}
+
+/* allocate hardware counter and optionally start counting */
+static int riscv_pmu_add(struct perf_event *event, int flags)
+{
+	struct pmu_hw_events *hw_events = this_cpu_ptr(riscv_pmu.hw_events);
+	struct hw_perf_event *hwc = &event->hw;
+
+	hw_events->events[hwc->idx] = event;
+
+	hwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;
+
+	if (flags & PERF_EF_START)
+		riscv_pmu_start(event, PERF_EF_RELOAD);
+
+	perf_event_update_userpage(event);
+
+	return 0;
+}
+
+irqreturn_t riscv_pmu_handle_irq(void)
+{
+	struct perf_sample_data data;
+	struct pmu_hw_events *cpuc = this_cpu_ptr(riscv_pmu.hw_events);
+	struct pt_regs *regs;
+	int idx;
+
+	/*
+	 * Did an overflow occur?
+	 */
+	if (!csr_read(SCOUNTEROF))
+		return IRQ_NONE;
+
+	/*
+	 * Handle the counter(s) overflow(s)
+	 */
+	regs = get_irq_regs();
+
+	for (idx = 0; idx < RISCV_MAX_COUNTERS; ++idx) {
+		struct perf_event *event = cpuc->events[idx];
+		struct hw_perf_event *hwc;
+
+		/* Ignore if we don't have an event. */
+		if (!event)
+			continue;
+		/*
+		 * We have a single interrupt for all counters. Check that
+		 * each counter has overflowed before we process it.
+		 */
+		if (!(csr_read(SCOUNTEROF) & BIT(idx)))
+			continue;
+
+		hwc = &event->hw;
+		riscv_perf_event_update(event, &event->hw);
+		perf_sample_data_init(&data, 0, hwc->last_period);
+		riscv_pmu_event_set_period(event);
+
+		if (perf_event_overflow(event, &data, regs))
+			riscv_pmu_stop_event(event);
+	}
+
+	/*
+	 * Handle the pending perf events.
+	 *
+	 * Note: this call *must* be run with interrupts disabled. For
+	 * platforms that can have the PMU interrupts raised as an NMI, this
+	 * will not work.
+	 */
+	irq_work_run();
+
+	return IRQ_HANDLED;
+}
+
+static void riscv_pmu_free_irq(void)
+{
+	int irq;
+	struct platform_device *pmu_device = riscv_pmu.plat_device;
+
+	irq = platform_get_irq(pmu_device, 0);
+	if (irq >= 0)
+		free_percpu_irq(irq, this_cpu_ptr(riscv_pmu.hw_events));
+}
+
+static int init_hw_perf_events(void)
+{
+	riscv_pmu.hw_events = alloc_percpu_gfp(struct pmu_hw_events,
+					      GFP_KERNEL);
+	if (!riscv_pmu.hw_events) {
+		pr_info("failed to allocate per-cpu PMU data.\n");
+		return -ENOMEM;
+	}
+
+	riscv_pmu.pmu = (struct pmu) {
+		.pmu_enable     = riscv_pmu_enable,
+		.pmu_disable    = riscv_pmu_disable,
+		.event_init     = riscv_pmu_event_init,
+		.add	    = riscv_pmu_add,
+		.del	    = riscv_pmu_del,
+		.start	  = riscv_pmu_start,
+		.stop	   = riscv_pmu_stop,
+		.read	   = riscv_pmu_read,
+	};
+
+	return 0;
+}
+
+static int riscv_pmu_starting_cpu(unsigned int cpu)
+{
+	sbi_set_pmu(1);
+	csr_set(sie, SIE_SMIE);
+	return 0;
+}
+
+static int riscv_pmu_dying_cpu(unsigned int cpu)
+{
+	csr_clear(sie, SIE_SMIE);
+	return 0;
+}
+
+int riscv_pmu_device_probe(struct platform_device *pdev,
+			  const struct of_device_id *of_table)
+{
+	int ret;
+
+	ret = init_hw_perf_events();
+	if (ret) {
+		pr_notice("[perf] failed to probe PMU!\n");
+		return ret;
+	}
+	riscv_pmu.max_period = ULONG_MAX;
+	riscv_pmu.plat_device = pdev;
+
+	ret = cpuhp_setup_state(CPUHP_AP_PERF_RISCV_ONLINE, "perf riscv:online",
+				riscv_pmu_starting_cpu,
+				riscv_pmu_dying_cpu);
+	if (ret) {
+		riscv_pmu_free_irq();
+		free_percpu(riscv_pmu.hw_events);
+		return ret;
+	}
+
+	ret = perf_pmu_register(&riscv_pmu.pmu, "c910_pmu", PERF_TYPE_RAW);
+	if (ret) {
+		riscv_pmu_free_irq();
+		free_percpu(riscv_pmu.hw_events);
+	}
+
+	return ret;
+}
+
+const static struct of_device_id riscv_pmu_of_device_ids[] = {
+	{.compatible = "riscv,c910_pmu"},
+	{},
+};
+
+static int riscv_pmu_dev_probe(struct platform_device *pdev)
+{
+	return riscv_pmu_device_probe(pdev, riscv_pmu_of_device_ids);
+}
+
+static struct platform_driver riscv_pmu_driver = {
+	.driver = {
+		   .name = "c910_pmu",
+		   .of_match_table = riscv_pmu_of_device_ids,
+		   },
+	.probe = riscv_pmu_dev_probe,
+};
+
+int __init riscv_pmu_probe(void)
+{
+	int ret;
+
+	ret = platform_driver_register(&riscv_pmu_driver);
+	if (ret)
+		pr_notice("[perf] PMU initialization failed\n");
+	else
+		pr_notice("[perf] PMU initialization done\n");
+
+	return ret;
+}
+device_initcall(riscv_pmu_probe);
diff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h
index 2d55cee63..168b23173 100644
--- a/include/linux/cpuhotplug.h
+++ b/include/linux/cpuhotplug.h
@@ -174,6 +174,7 @@ enum cpuhp_state {
 	CPUHP_AP_PERF_POWERPC_CORE_IMC_ONLINE,
 	CPUHP_AP_PERF_POWERPC_THREAD_IMC_ONLINE,
 	CPUHP_AP_PERF_POWERPC_TRACE_IMC_ONLINE,
+	CPUHP_AP_PERF_RISCV_ONLINE,
 	CPUHP_AP_WATCHDOG_ONLINE,
 	CPUHP_AP_WORKQUEUE_ONLINE,
 	CPUHP_AP_RCUTREE_ONLINE,
-- 
2.17.1

