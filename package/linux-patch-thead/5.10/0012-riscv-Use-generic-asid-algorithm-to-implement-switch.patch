From 31b10e57d74f85dfdf4280fe7fe18298404a17e8 Mon Sep 17 00:00:00 2001
From: Guo Ren <guoren@linux.alibaba.com>
Date: Wed, 9 Oct 2019 15:20:26 +0800
Subject: [PATCH 12/59] riscv: Use generic asid algorithm to implement
 switch_mm

The patch is based on (riscv: Add new asid lib code from arm). We
use asid to reduce tlb flush. T-HEAD C910 rv64GVC could broadcast
tlb operations to sibling harts. So we unify asid, just like arm64.

Signed-off-by: Guo Ren <guoren@linux.alibaba.com>
---
 arch/riscv/include/asm/csr.h         |  3 ++
 arch/riscv/include/asm/mmu.h         |  1 +
 arch/riscv/include/asm/mmu_context.h | 17 ++++++-----
 arch/riscv/include/asm/pgtable.h     |  1 +
 arch/riscv/mm/Makefile               |  1 +
 arch/riscv/mm/context.c              | 43 ++++++++++++++++++++++++++--
 6 files changed, 57 insertions(+), 9 deletions(-)

diff --git a/arch/riscv/include/asm/csr.h b/arch/riscv/include/asm/csr.h
index cec462e198ce..e052a829d1e7 100644
--- a/arch/riscv/include/asm/csr.h
+++ b/arch/riscv/include/asm/csr.h
@@ -45,6 +45,9 @@
 #define SATP_PPN	_AC(0x00000FFFFFFFFFFF, UL)
 #define SATP_MODE_39	_AC(0x8000000000000000, UL)
 #define SATP_MODE	SATP_MODE_39
+#define SATP_ASID_BITS	16
+#define SATP_ASID_SHIFT	44
+#define SATP_ASID_MASK	_AC(0xFFFF, UL)
 #endif
 
 /* Exception cause high bit - is an interrupt if set */
diff --git a/arch/riscv/include/asm/mmu.h b/arch/riscv/include/asm/mmu.h
index dabcf2cfb3dc..c78e0741992b 100644
--- a/arch/riscv/include/asm/mmu.h
+++ b/arch/riscv/include/asm/mmu.h
@@ -14,6 +14,7 @@ typedef struct {
 	unsigned long	end_brk;
 #endif
 	void *vdso;
+	atomic64_t asid;
 #ifdef CONFIG_SMP
 	/* A local icache flush is needed before user execution can resume. */
 	cpumask_t icache_stale_mask;
diff --git a/arch/riscv/include/asm/mmu_context.h b/arch/riscv/include/asm/mmu_context.h
index 67c463812e2d..5f44a8e0859b 100644
--- a/arch/riscv/include/asm/mmu_context.h
+++ b/arch/riscv/include/asm/mmu_context.h
@@ -12,19 +12,20 @@
 
 #include <linux/mm.h>
 #include <linux/sched.h>
+#include <asm/tlbflush.h>
+#include <asm/cacheflush.h>
+#include <asm/asid.h>
+
+#define ASID_MASK		((1 << SATP_ASID_BITS) - 1)
+#define cpu_asid(mm)		(atomic64_read(&mm->context.asid) & ASID_MASK)
+
+#define init_new_context(tsk,mm)	({ atomic64_set(&(mm)->context.asid, 0); 0; })
 
 static inline void enter_lazy_tlb(struct mm_struct *mm,
 	struct task_struct *task)
 {
 }
 
-/* Initialize context-related info for a new mm_struct */
-static inline int init_new_context(struct task_struct *task,
-	struct mm_struct *mm)
-{
-	return 0;
-}
-
 static inline void destroy_context(struct mm_struct *mm)
 {
 }
@@ -32,6 +33,8 @@ static inline void destroy_context(struct mm_struct *mm)
 void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	struct task_struct *task);
 
+void check_and_switch_context(struct mm_struct *mm, unsigned int cpu);
+
 static inline void activate_mm(struct mm_struct *prev,
 			       struct mm_struct *next)
 {
diff --git a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h
index ff2b01769ebd..f3c9f9a1c1bb 100644
--- a/arch/riscv/include/asm/pgtable.h
+++ b/arch/riscv/include/asm/pgtable.h
@@ -97,6 +97,7 @@
 #define _PAGE_KERNEL		(_PAGE_READ \
 				| _PAGE_WRITE \
 				| _PAGE_PRESENT \
+				| _PAGE_GLOBAL \
 				| _PAGE_ACCESSED \
 				| _PAGE_DIRTY \
 				| _PAGE_CACHE \
diff --git a/arch/riscv/mm/Makefile b/arch/riscv/mm/Makefile
index bca8bf99af97..70cca3ff225c 100644
--- a/arch/riscv/mm/Makefile
+++ b/arch/riscv/mm/Makefile
@@ -16,6 +16,7 @@ obj-y += context.o
 obj-y += dma-mapping.o
 obj-y += ioremap.o
 obj-y += asid.o
+obj-y += context.o
 
 ifeq ($(CONFIG_MMU),y)
 obj-$(CONFIG_SMP) += tlbflush.o
diff --git a/arch/riscv/mm/context.c b/arch/riscv/mm/context.c
index 613ec81a8979..06a900574b70 100644
--- a/arch/riscv/mm/context.c
+++ b/arch/riscv/mm/context.c
@@ -44,6 +44,7 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	struct task_struct *task)
 {
 	unsigned int cpu;
+	unsigned long asid;
 
 	if (unlikely(prev == next))
 		return;
@@ -59,9 +60,47 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	cpumask_set_cpu(cpu, mm_cpumask(next));
 
 #ifdef CONFIG_MMU
-	csr_write(CSR_SATP, virt_to_pfn(next->pgd) | SATP_MODE);
-	local_flush_tlb_all();
+	check_and_switch_context(next, cpu);
+	asid = (next->context.asid.counter & SATP_ASID_MASK)
+		<< SATP_ASID_SHIFT;
+
+	csr_write(sptbr, virt_to_pfn(next->pgd) | SATP_MODE | asid);
 #endif
 
 	flush_icache_deferred(next);
 }
+
+static DEFINE_PER_CPU(atomic64_t, active_asids);
+static DEFINE_PER_CPU(u64, reserved_asids);
+
+struct asid_info asid_info;
+
+void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
+{
+	asid_check_context(&asid_info, &mm->context.asid, cpu, mm);
+}
+
+static void asid_flush_cpu_ctxt(void)
+{
+	local_flush_tlb_all();
+}
+
+static int asids_init(void)
+{
+	BUG_ON(((1 << SATP_ASID_BITS) - 1) <= num_possible_cpus());
+
+	if (asid_allocator_init(&asid_info, SATP_ASID_BITS, 1,
+				asid_flush_cpu_ctxt))
+		panic("Unable to initialize ASID allocator for %lu ASIDs\n",
+		      NUM_ASIDS(&asid_info));
+
+	asid_info.active = &active_asids;
+	asid_info.reserved = &reserved_asids;
+
+	pr_info("ASID allocator initialised with %lu entries\n",
+		NUM_CTXT_ASIDS(&asid_info));
+
+	local_flush_tlb_all();
+	return 0;
+}
+early_initcall(asids_init);
-- 
2.17.1

